"""
üß† Multimodal Fusion to Mitigate Hallucination in LLMs
Ultimate Research-Grade Edition ‚Äî Wikipedia ‚Ä¢ Captioning ‚Ä¢ Contradiction ‚Ä¢ Metrics (Fixed)
"""

import streamlit as st
from PIL import Image
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import torch, numpy as np, matplotlib.pyplot as plt
import PyPDF2, google.generativeai as genai, wikipediaapi
from transformers import BlipProcessor, BlipForConditionalGeneration
from dotenv import load_dotenv
import os
from gtts import gTTS
import io

# ------------------------------------------------------------
# ENV + PAGE CONFIG
# ------------------------------------------------------------
load_dotenv()
gemini_key = os.getenv("GEMINI_API_KEY")

st.set_page_config(page_title="Multimodal Hallucination Mitigation", layout="wide")
st.title("üß† Multimodal Fusion to Mitigate Hallucination in LLMs ‚Äî Research-Enhanced Edition")
st.caption("Gemini-2.5-Flash ‚Ä¢ Wikipedia ‚Ä¢ Image Captioning ‚Ä¢ Contradiction ‚Ä¢ Detection ‚Ä¢ Voice Narration")

# ------------------------------------------------------------
# SIDEBAR SETTINGS
# ------------------------------------------------------------
st.sidebar.header("‚öôÔ∏è Settings")
fusion_type = st.sidebar.selectbox("Fusion Strategy",
                                   ["Early Fusion", "Late Fusion", "Cross-Modal Attention", "Retrieval-Augmented Fusion"])
domain = st.sidebar.selectbox("Domain", ["medical", "ecommerce", "news", "general"])
show_reasoning = st.sidebar.checkbox("üß© Show Gemini Reasoning", value=True)
voice_output = st.sidebar.checkbox("üîä Voice Narration", value=False)
use_gemini = bool(gemini_key)

# ------------------------------------------------------------
# KNOWLEDGE BASE
# ------------------------------------------------------------
knowledge_text = """
Multimodal fusion integrates text, image, and structured knowledge to reduce hallucinations in LLMs.
Cross-modal attention aligns words and visual regions to ensure factual grounding.
Retrieval-augmented generation (RAG) verifies claims before producing output.
Hallucinations are intrinsic, extrinsic, contextual, and multimodal.
Use cases include medical diagnostics, e-commerce product QA, and news fact verification.
Evaluation metrics: faithfulness, groundedness, hallucination reduction.
"""
vectorizer = TfidfVectorizer().fit([knowledge_text])

# ------------------------------------------------------------
# HELPER FUNCTIONS
# ------------------------------------------------------------
def text_only_model(query, domain):
    examples = {
        "medical": "‚ùå The X-ray shows lung cancer (incorrect hallucination).",
        "ecommerce": "‚ùå This phone supports wireless charging (fabricated).",
        "news": "‚ùå Aliens discovered near Mars base (false).",
        "general": "‚ùå The person in the picture is a scientist (assumed)."
    }
    return examples.get(domain, "‚ùå Text-only model may hallucinate details.")

def extract_pdf_text(uploaded_file):
    try:
        text = ""
        reader = PyPDF2.PdfReader(uploaded_file)
        for page in reader.pages:
            text += page.extract_text() or ""
        return " ".join(text.split())[:3000]
    except Exception as e:
        return f"[Error reading PDF: {e}]"

def extract_top_evidence(query, pdf_text, top_k=5):
    sentences = [s.strip() for s in pdf_text.split(".") if s.strip()]
    if not sentences:
        return []
    encoder = SentenceTransformer("all-MiniLM-L6-v2")
    q_vec = encoder.encode([query])
    s_vecs = encoder.encode(sentences)
    sims = cosine_similarity(q_vec, s_vecs)[0]
    idxs = np.argsort(sims)[::-1][:top_k]
    return [(sentences[i], float(sims[i])) for i in idxs]

def get_wikipedia_evidence(query, sentences=3):
    # ‚úÖ FIXED: proper user agent (Wikipedia requires one)
    wiki = wikipediaapi.Wikipedia(
        user_agent="Multimodal-Hallucination-Demo/1.0 (pranshu.ai.project@example.com)",
        language="en"
    )
    page = wiki.page(query.split()[0])
    if not page.exists():
        return "No relevant Wikipedia evidence found."
    text = page.summary.split(".")
    return ". ".join(text[:sentences]) + "."

def generate_image_caption(image):
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    inputs = processor(image, return_tensors="pt")
    out = model.generate(**inputs, max_new_tokens=30)
    return processor.decode(out[0], skip_special_tokens=True)

def detect_hallucination(response, pdf_text, wiki_text):
    """Sentence-level hallucination detection"""
    sentences = [s for s in response.split(".") if len(s) > 10]
    encoder = SentenceTransformer("all-MiniLM-L6-v2")
    ev_text = pdf_text + " " + wiki_text + " " + knowledge_text
    ev_vec = encoder.encode(ev_text, convert_to_tensor=True)
    hallucinated = []
    for s in sentences:
        s_vec = encoder.encode(s, convert_to_tensor=True)
        sim = torch.nn.functional.cosine_similarity(s_vec, ev_vec, dim=0).item()
        if sim < 0.25:
            hallucinated.append(s.strip())
    return hallucinated

def multimodal_model(query, image=None, pdf_text=None, wiki_text=""):
    if use_gemini:
        try:
            genai.configure(api_key=gemini_key)
            model = genai.GenerativeModel("gemini-2.5-flash")
            prompt = f"""
You are a hallucination-resistant multimodal AI.
Verify the following query using all available evidence (image, text, PDF, Wikipedia).
Be factual, concise, and cite evidence sources.
Query: {query}
"""
            if show_reasoning:
                prompt += "\nExplain reasoning briefly and note which modality supported the conclusion."

            inputs = [prompt]
            if image: inputs.append(image)
            if pdf_text: inputs.append({"mime_type":"text/plain","data":pdf_text})
            if wiki_text: inputs.append({"mime_type":"text/plain","data":wiki_text})

            try:
                resp = model.generate_content(inputs)
                if not resp.text: raise ValueError("Empty response.")
                return "‚úÖ Gemini 2.5 Flash Grounded Response:\n\n" + resp.text.strip()
            except Exception:
                resp = model.generate_content([prompt, image] if image else [prompt])
                return "‚úÖ Gemini (Retry Without PDF):\n\n" + (resp.text or "No response.")
        except Exception as e:
            return f"‚ö†Ô∏è Gemini API Error handled safely: {e}\nSwitching to local verification."

    # ---------- LOCAL MODE ----------
    encoder = SentenceTransformer("clip-ViT-B-32")
    qv = encoder.encode(query, convert_to_tensor=True)
    kv = encoder.encode(knowledge_text, convert_to_tensor=True)
    sim = torch.nn.functional.cosine_similarity(qv, kv, dim=0).item()
    if sim > 0.25:
        return f"‚úÖ Local Grounded Response: Evidence supports that *{query}* is consistent."
    else:
        return f"‚ö†Ô∏è Local Grounded Response: Limited evidence but hallucination risk reduced."

# ------------------------------------------------------------
# USER INPUT
# ------------------------------------------------------------
query = st.text_input("üí¨ Enter your question:", "Does this X-ray show pneumonia?")
img_file = st.file_uploader("üì∑ Upload an image (optional)", type=["png","jpg","jpeg"])
pdf_file = st.file_uploader("üìÑ Upload a PDF document (optional)", type=["pdf"])

pdf_text=""
if pdf_file:
    with st.expander("üìë Extracted PDF Text (click to view)"):
        pdf_text = extract_pdf_text(pdf_file)
        st.write(pdf_text if pdf_text else "No extractable text found.")

# Wikipedia retrieval
wiki_text = get_wikipedia_evidence(query)
with st.expander("üåê Wikipedia Evidence (Auto-Fetched)"):
    st.write(wiki_text)

# Image captioning
caption_text = ""
if img_file:
    image_for_model = Image.open(img_file).convert("RGB")
    caption_text = generate_image_caption(image_for_model)
    with st.expander("üñºÔ∏è Image Caption (Generated)"):
        st.write(f"**Caption:** {caption_text}")

# ------------------------------------------------------------
# MAIN BUTTON
# ------------------------------------------------------------
if st.button("üöÄ Run Multimodal Verification"):
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üß† Model Comparison",
        "üìä Analysis",
        "üîç Evidence Viewer",
        "üìà Metrics Radar",
        "‚ö†Ô∏è Hallucination Detection"
    ])

    # ---- TAB 1 ----
    with tab1:
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("‚ùå Text-only LLM (Hallucination Risk)")
            st.warning(text_only_model(query, domain))
        with col2:
            st.subheader("‚úÖ Multimodal Grounded Model")
            if img_file: st.image(image_for_model, caption="Uploaded Image", width=280)
            result = multimodal_model(query, image=image_for_model if img_file else None, pdf_text=pdf_text, wiki_text=wiki_text)
            st.success(result)
            if voice_output:
                tts = gTTS(text=result, lang='en')
                audio_bytes = io.BytesIO()
                tts.write_to_fp(audio_bytes)
                st.audio(audio_bytes.getvalue(), format='audio/mp3')
            st.download_button("üì• Download Response", result.encode(), "response.txt")

    # ---- TAB 2 ----
    with tab2:
        q_vec = vectorizer.transform([query])
        kb_vec = vectorizer.transform([knowledge_text + pdf_text + wiki_text])
        sim_score = cosine_similarity(q_vec, kb_vec)[0][0]
        hallucination_score = max(0, 1 - sim_score)
        st.metric("Evidence Similarity", f"{sim_score:.2f}")
        st.metric("Estimated Hallucination Probability", f"{hallucination_score*100:.1f}%")
        st.progress(min(sim_score, 1.0))

        labels = ["Text","Image","PDF","Wikipedia"]
        weights = np.array([0.3,0.2,0.2,0.3])
        fig, ax = plt.subplots(figsize=(3,3))
        ax.pie(weights, labels=labels, autopct='%1.1f%%', startangle=140)
        st.pyplot(fig)

    # ---- TAB 3 ----
    with tab3:
        if pdf_text:
            st.subheader("üìÑ Top PDF Evidence")
            for sent,score in extract_top_evidence(query,pdf_text):
                st.write(f"‚Ä¢ {sent} ‚Äî **Similarity:** {score:.2f}")
        else:
            st.info("Upload a PDF to view extracted evidence.")
        st.subheader("üåê Wikipedia Summary")
        st.write(wiki_text)

    # ---- TAB 4 ----
    with tab4:
        metrics = {
            "Faithfulness": round(sim_score*0.9,2),
            "Groundedness": round(sim_score,2),
            "Relevance": round(min(1,sim_score+0.1),2),
            "Fluency": 0.9
        }
        labels = list(metrics.keys())
        values = list(metrics.values()) + [list(metrics.values())[0]]
        angles = np.linspace(0,2*np.pi,len(labels),endpoint=False).tolist() + [0]
        fig, ax = plt.subplots(subplot_kw={'projection':'polar'})
        ax.plot(angles, values, 'o-', linewidth=2)
        ax.fill(angles, values, alpha=0.25)
        ax.set_thetagrids(np.degrees(angles[:-1]), labels)
        ax.set_title("Radar Chart: Model Evaluation Metrics", y=1.1)
        st.pyplot(fig)

    # ---- TAB 5 ----
    with tab5:
        st.subheader("‚ö†Ô∏è Sentence-Level Hallucination Detection")
        hallucinated = detect_hallucination(result, pdf_text, wiki_text)
        if not hallucinated:
            st.success("No hallucinations detected in the grounded response ‚úÖ")
        else:
            for h in hallucinated:
                st.markdown(f"<span style='color:red'>‚Ä¢ {h}</span>", unsafe_allow_html=True)

    # Footer
    st.markdown("---")
    st.success("üéâ Multimodal Fusion Complete ‚Äî Hallucination Mitigated & Analyzed Successfully!")
else:
    st.info("üëà Enter a query, upload optional image/PDF, and click **Run Multimodal Verification** to start.")
